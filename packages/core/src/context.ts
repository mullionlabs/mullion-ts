import type { Owned } from './owned.js';

/**
 * Context represents an execution scope for LLM operations.
 *
 * A Context provides a type-safe container for LLM inference operations within
 * a specific scope. It ensures that all values generated within this context
 * are properly tagged with the scope identifier, enabling compile-time detection
 * of context leaks.
 *
 * The scope parameter `S` is a string literal type that uniquely identifies
 * this context at compile time, preventing accidental mixing of data from
 * different security boundaries or execution contexts.
 *
 * @template S - The scope identifier (string literal type for compile-time safety)
 *
 * @example
 * ```typescript
 * // Create contexts for different security boundaries
 * const adminCtx: Context<'admin'> = createContext('admin');
 * const customerCtx: Context<'customer'> = createContext('customer');
 *
 * // Generate values in admin scope
 * const adminNotes = await adminCtx.infer(NotesSchema, document);
 * // adminNotes has type: Owned<Notes, 'admin'>
 *
 * // Cannot accidentally use admin data in customer context (compile-time error)
 * await customerCtx.use(adminNotes); // ❌ Type error
 *
 * // Must explicitly bridge to cross scopes
 * const bridged = customerCtx.bridge(adminNotes);
 * // bridged has type: Owned<Notes, 'admin' | 'customer'>
 * await customerCtx.use(bridged); // ✅ OK
 * ```
 *
 * @see {@link Owned} Values generated by LLM operations
 * @see {@link SemanticValue} Values with alternatives and reasoning
 */
export interface Context<S extends string> {
  /**
   * The scope identifier for this context.
   *
   * This readonly property contains the string literal that identifies
   * the context's scope at both compile-time and runtime.
   *
   * @example
   * ```typescript
   * const ctx: Context<'user-input'> = createContext('user-input');
   * console.log(ctx.scope); // 'user-input'
   * ```
   */
  readonly scope: S;

  /**
   * Infer a typed value from unstructured input using an LLM.
   *
   * This method takes unstructured input (like natural language text) and
   * uses an LLM to extract structured data according to the provided schema.
   * The result is wrapped in an {@link Owned} type tagged with this context's scope.
   *
   * @template T - The type of value to infer
   * @param schema - Schema definition for the expected output type
   * @param input - Unstructured input to process (e.g., user message, document)
   * @param options - Optional configuration for the inference operation
   * @returns Promise resolving to an Owned value in this scope
   *
   * @example
   * ```typescript
   * import { z } from 'zod';
   *
   * const EmailSchema = z.object({
   *   subject: z.string(),
   *   category: z.enum(['support', 'sales', 'feedback']),
   *   priority: z.enum(['low', 'medium', 'high'])
   * });
   *
   * const emailCtx: Context<'email-processing'> = createContext('email-processing');
   *
   * const structured = await emailCtx.infer(EmailSchema, rawEmailText);
   * // structured: Owned<Email, 'email-processing'>
   *
   * if (structured.confidence > 0.8) {
   *   console.log(`Category: ${structured.value.category}`);
   *   console.log(`Priority: ${structured.value.priority}`);
   * }
   * ```
   *
   * @example
   * ```typescript
   * // Extracting user intent from natural language
   * const UserIntentSchema = z.object({
   *   action: z.enum(['create', 'update', 'delete', 'query']),
   *   entity: z.string(),
   *   parameters: z.record(z.unknown())
   * });
   *
   * const chatCtx: Context<'chat'> = createContext('chat');
   *
   * const intent = await chatCtx.infer(
   *   UserIntentSchema,
   *   "Can you update my profile picture?"
   * );
   *
   * console.log(intent.value.action); // 'update'
   * console.log(intent.value.entity); // 'profile_picture'
   * ```
   */
  infer<T>(
    schema: Schema<T>,
    input: string,
    options?: InferOptions
  ): Promise<Owned<T, S>>;

  /**
   * Bridge a value from another scope into this context.
   *
   * Bridging is the explicit, trackable way to transfer data between different
   * scopes. When you bridge a value, the resulting type is a union of both scopes,
   * making the cross-scope data flow visible in the type system.
   *
   * This prevents accidental context leaks while allowing intentional data sharing
   * with full type safety and auditability.
   *
   * @template T - The type of the value being bridged
   * @template OS - The source scope of the value (Other Scope)
   * @param owned - The Owned value from another scope to bridge
   * @returns An Owned value that carries both source and target scope information
   *
   * @example
   * ```typescript
   * // Admin context generates sensitive data
   * const adminCtx: Context<'admin'> = createContext('admin');
   * const adminNotes = await adminCtx.infer(NotesSchema, document);
   * // adminNotes: Owned<Notes, 'admin'>
   *
   * // Customer context needs to use that data
   * const customerCtx: Context<'customer'> = createContext('customer');
   *
   * // ❌ BAD: Direct use causes compile error
   * // customerCtx.use(adminNotes); // Type error
   *
   * // ✅ GOOD: Explicit bridge makes data flow visible
   * const bridged = customerCtx.bridge(adminNotes);
   * // bridged: Owned<Notes, 'admin' | 'customer'>
   *
   * // Now it's safe to use - bridge is tracked in the type
   * customerCtx.use(bridged);
   * ```
   *
   * @example
   * ```typescript
   * // Bridging through multiple contexts
   * const inputCtx: Context<'user-input'> = createContext('user-input');
   * const processCtx: Context<'processing'> = createContext('processing');
   * const outputCtx: Context<'output'> = createContext('output');
   *
   * const userQuery = await inputCtx.infer(QuerySchema, userMessage);
   * // userQuery: Owned<Query, 'user-input'>
   *
   * const processed = processCtx.bridge(userQuery);
   * // processed: Owned<Query, 'user-input' | 'processing'>
   *
   * const final = outputCtx.bridge(processed);
   * // final: Owned<Query, 'user-input' | 'processing' | 'output'>
   * // Full provenance chain visible in type!
   * ```
   */
  bridge<T, OS extends string>(owned: Owned<T, OS>): Owned<T, S | OS>;

  /**
   * Use a value that belongs to this scope.
   *
   * This method extracts the raw value from an Owned wrapper, but only if
   * the value's scope matches this context's scope. This provides a type-safe
   * way to unwrap values while preventing use of values from incompatible scopes.
   *
   * @template T - The type of the value
   * @param owned - An Owned value from this scope (or a bridged value that includes this scope)
   * @returns The unwrapped value
   *
   * @example
   * ```typescript
   * const ctx: Context<'processing'> = createContext('processing');
   *
   * const data = await ctx.infer(DataSchema, input);
   * // data: Owned<Data, 'processing'>
   *
   * const rawValue = ctx.use(data);
   * // rawValue: Data (unwrapped)
   *
   * // Now you can work with the raw value
   * console.log(rawValue.field1);
   * console.log(rawValue.field2);
   * ```
   *
   * @example
   * ```typescript
   * // Type-safe usage with scope checking
   * const adminCtx: Context<'admin'> = createContext('admin');
   * const userCtx: Context<'user'> = createContext('user');
   *
   * const adminData = await adminCtx.infer(DataSchema, input);
   * // adminData: Owned<Data, 'admin'>
   *
   * // ❌ This won't compile - scope mismatch
   * // const value = userCtx.use(adminData); // Type error
   *
   * // ✅ Must bridge first
   * const bridged = userCtx.bridge(adminData);
   * const value = userCtx.use(bridged); // OK
   * ```
   *
   * @example
   * ```typescript
   * // Checking confidence before use
   * const ctx: Context<'validation'> = createContext('validation');
   * const result = await ctx.infer(ResultSchema, data);
   *
   * if (result.confidence < 0.8) {
   *   throw new Error(`Low confidence: ${result.confidence}`);
   * }
   *
   * const validated = ctx.use(result);
   * // Safe to use - confidence was checked
   * ```
   */
  use<T>(owned: Owned<T, S>): T;
}

/**
 * Schema definition for LLM inference operations.
 *
 * This is a placeholder interface that will be implemented by integration packages
 * (like @scopestack/ai-sdk). It represents a schema that can describe the
 * structure of data to extract from unstructured input.
 *
 * @template T - The TypeScript type that the schema describes
 *
 * @example
 * ```typescript
 * // Using Zod schemas (from @scopestack/ai-sdk)
 * import { z } from 'zod';
 *
 * const UserSchema: Schema<User> = z.object({
 *   name: z.string(),
 *   age: z.number(),
 *   email: z.string().email()
 * });
 * ```
 */
export interface Schema<T> {
  // Placeholder - will be defined by integration packages
  readonly _type?: T;
}

/**
 * Configuration options for the Context.
 *
 * These options control the behavior of the context and its LLM operations.
 *
 * @example
 * ```typescript
 * const options: ContextOptions = {
 *   provider: openai('gpt-4'),
 *   temperature: 0.7,
 *   maxTokens: 2000,
 *   metadata: {
 *     userId: '123',
 *     sessionId: 'abc'
 *   }
 * };
 * ```
 */
export interface ContextOptions {
  /**
   * LLM provider configuration.
   *
   * Specifies which LLM to use for inference operations.
   * The exact type depends on the integration package.
   *
   * @example
   * ```typescript
   * // Using Vercel AI SDK (from @scopestack/ai-sdk)
   * import { openai } from '@ai-sdk/openai';
   *
   * const options: ContextOptions = {
   *   provider: openai('gpt-4')
   * };
   * ```
   */
  provider?: unknown;

  /**
   * Temperature for LLM sampling (0-2).
   *
   * - Lower values (0-0.5): More deterministic, focused outputs
   * - Medium values (0.5-1.0): Balanced creativity and consistency
   * - Higher values (1.0-2.0): More creative, diverse outputs
   *
   * Defaults to provider's default (typically 1.0).
   */
  temperature?: number;

  /**
   * Maximum number of tokens to generate.
   *
   * Controls the maximum length of the LLM's response.
   * Defaults to provider's default.
   */
  maxTokens?: number;

  /**
   * Additional metadata to attach to all operations in this context.
   *
   * Useful for logging, debugging, and tracking requests.
   *
   * @example
   * ```typescript
   * const options: ContextOptions = {
   *   metadata: {
   *     userId: '12345',
   *     sessionId: 'session-abc',
   *     version: 'v2.0'
   *   }
   * };
   * ```
   */
  metadata?: Record<string, unknown>;
}

/**
 * Options for a single inference operation.
 *
 * These options can override context-level settings for individual infer() calls.
 *
 * @example
 * ```typescript
 * // Override temperature for this specific inference
 * const result = await ctx.infer(
 *   Schema,
 *   input,
 *   { temperature: 0.3 } // Lower temperature for more focused output
 * );
 * ```
 */
export interface InferOptions {
  /**
   * Override the context's temperature for this inference.
   */
  temperature?: number;

  /**
   * Override the context's max tokens for this inference.
   */
  maxTokens?: number;

  /**
   * System prompt to guide the LLM's behavior.
   *
   * @example
   * ```typescript
   * const result = await ctx.infer(IntentSchema, userMessage, {
   *   systemPrompt: 'You are a helpful customer service assistant. Be concise and professional.'
   * });
   * ```
   */
  systemPrompt?: string;

  /**
   * Whether to return a SemanticValue with alternatives and reasoning.
   *
   * When true, the LLM will provide alternative interpretations and
   * explain its reasoning for the chosen value.
   *
   * @default false
   *
   * @example
   * ```typescript
   * const result = await ctx.infer(CategorySchema, text, {
   *   includeAlternatives: true
   * });
   *
   * // result is now SemanticValue<Category, S> instead of Owned<Category, S>
   * console.log(result.reasoning);
   * console.log(result.alternatives);
   * ```
   */
  includeAlternatives?: boolean;

  /**
   * Additional metadata for this specific inference.
   */
  metadata?: Record<string, unknown>;
}
